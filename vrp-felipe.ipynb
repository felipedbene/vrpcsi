{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing vehicle delivery optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Proof of Concept that applying Reinforcement Learning (RL) to solve the classical vehícle routing problem could potentially help our customer Oktank to optimize the delivery of its products and generate savings through the use of sagemaker capabilities and pre-loaded machine learning algorythms.\n",
    "\n",
    "Having the model defined, customer data will be fed into the model this way we can measure the pòtential gain with this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a delivery driver using a phone app, orders arrive on the app in a dynamic manner. Each order has a delivery charge known to the driver at the time of order creation, and it is assigned to a location in the country(Mexico). The country is a grid map and consists of mutually exclusive zones that generate orders at different rates and rewards. The orders have a delivery time limit, the timer starts with the order creation and is same for all orders. The driver has to accept an order and pick up the cars from a given location prior to delivery. The truck has a capacity limit of 13, but the driver can accept unlimited orders and plan their route accordingly. The driver’s goal is to maximize the total net reward.\n",
    "\n",
    "This formulation is known as stochastic and dynamic capacitated vehicle routing problem with pickup and delivery, time windows and service guarantee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Model.png\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, the RL agent is aware of the following information:\n",
    "\n",
    "    Pickup location\n",
    "    Driver info: driver's location, capacity left\n",
    "    Order info: order's location, order's status (open, accepted, picked up or delivered), time elapsed since each order’s generation, order dollar value\n",
    "\n",
    "At each time step, the RL agent can take one of the following actions:\n",
    "\n",
    "    Accept an order\n",
    "    Pick up an accepted order\n",
    "    Go to a dealer's node for delivery\n",
    "    Head to a specific pickup location\n",
    "    Wait and stay unmoved\n",
    "\n",
    "During training, the agent cannot perform the following invalid actions: (i) pick up an order when its remaining capacity is 0; (ii) pick up an order that is not yet accepted; (iii) deliver an order that is not yet picked up.\n",
    "\n",
    "At each time step, the reward is defined as the difference between total value of all delivered orders and cost:\n",
    "\n",
    "    Total value of all delivered orders is divided into 3 equal parts -- when the order gets accepted, picked up, and delivered respectively\n",
    "    Cost includes time cost and moving cost. Both are per time step\n",
    "    A large penalty is imposed if the agent accepts an order but fails to deliver within the delivery limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker for RL\n",
    "\n",
    "Amazon SageMaker allows you to train your RL agents in cloud machines using docker containers. You do not have to worry about setting up your machines with the RL toolkits and deep learning frameworks. You can easily switch between many different machines setup for you, including powerful GPU machines that give a big speedup. You can also choose to use multiple machines in a cluster to further speedup training, often necessary for production level loads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup S3 bucket\n",
    "\n",
    "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://vrp-sagemaker-us-east-1-803186506512/\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket\n",
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = \"vrp-\" + sage_session.default_bucket()\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables\n",
    "\n",
    "We define variables such as the job prefix for the training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique job name \n",
    "job_name_prefix = 'vrp-oktank-logistics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going with sagemaker mode, local present just for debugging.\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    # Pick the instance type used for trainning the model\n",
    "    instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::803186506512:role/service-role/AmazonSageMaker-ExecutionRole-20191125T103535\n"
     ]
    }
   ],
   "source": [
    "#Get the local role for invoking other AWS Services, such as S3 where the metadata will live.\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "# Display the current role.\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "The environment is defined in a Python file called src/vrp_env.py and the file is uploaded on /src directory.\n",
    "\n",
    "The environment also implements the init(), step() and reset() functions that describe how the environment behaves. This is consistent with Open AI Gym interfaces for defining an environment.\n",
    "\n",
    "1. init() - initialize the environment in a pre-defined state\n",
    "2.    step() - take an action on the environment\n",
    "3.    reset()- restart the environment on a new episode\n",
    "4.    render() - get a rendered image of the environment in its current state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize src/vrp_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the training code\n",
    "\n",
    "The training code is written in the file `train_vehicle_routing_problem.py` which is also uploaded in the /src directory. First we import the environment files and the preset files, and then define the main() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize src/train_vehicle_routing_problem.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. \n",
    "When using SageMaker for training, we can select a GPU instance.\n",
    "\n",
    "1. Specify the source directory where the gym environment and training code is uploaded (S3 Bucket defined above)\n",
    "2. Specify the entry point as the training code, the code we just went through above.\n",
    "3. Specify the choice of RL toolkit and framework. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm.\n",
    "6. Define the metrics definitions that we are interested in capturing in your logs (Cloudwatch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name': 'episode_reward_mean',\n",
    "  'Regex': 'episode_reward_mean: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'episode_reward_max',\n",
    "  'Regex': 'episode_reward_max: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'episode_reward_min',\n",
    "  'Regex': 'episode_reward_min: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entry_point = \"train_vehicle_routing_problem.py\"\n",
    "\n",
    "#How long we want to the train this model for\n",
    "train_job_max_duration_in_seconds = 60 * 10\n",
    "\n",
    "estimator = RLEstimator(entry_point= train_entry_point,\n",
    "                        source_dir=\"src\",\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.RAY,\n",
    "                        toolkit_version='0.6.5',\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        train_max_run=train_job_max_duration_in_seconds,\n",
    "                        hyperparameters={}\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job: vrp-oktank-logistics-2020-01-09-17-00-41-892\n",
      "2020-01-09 17:00:53 Starting - Starting the training job...\n",
      "2020-01-09 17:00:54 Starting - Launching requested ML instances."
     ]
    }
   ],
   "source": [
    "#Calling .fit() method will start the model trainning\n",
    "estimator.fit(wait=local_mode)\n",
    "rl_job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % rl_job_name)\n",
    "rl_estimator = RLEstimator.attach(rl_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 job path: s3://vrp-sagemaker-us-east-1-803186506512/vrp-oktank-logistics-2020-01-03-21-29-21-168\n",
      "Intermediate folder path: s3://vrp-sagemaker-us-east-1-803186506512/vrp-oktank-logistics-2020-01-03-21-29-21-168/output/intermediate/training/\n"
     ]
    }
   ],
   "source": [
    "#Visualize the trainning progress through intermediate folder creation\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,rl_job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(rl_job_name)\n",
    "intermediate_url = \"s3://{}/{}training/\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting the metrics for the trainning:\n",
    "It's important to notice that sage maker provides an analytic functions so we can use this data to see the model performance overtime.\n",
    "Below we see the reward over time and how it increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TrainingStartTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bc67bae3a779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward_min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward_max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_job_name, metric_names, sagemaker_session, start_time, end_time, period)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36mclear_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobAnalytics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_timeinterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_determine_timeinterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/analytics.py\u001b[0m in \u001b[0;36m_determine_timeinterval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m    339\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu\"TrainingStartTime\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# datetime object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Incrementing end time by 1 min since CloudWatch drops seconds before finding the logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# This results in logs being searched in the time range in which the correct log line was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TrainingStartTime'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "if not local_mode:\n",
    "    df = TrainingJobAnalytics(rl_job_name, ['episode_reward_mean']).dataframe()\n",
    "    df_min = TrainingJobAnalytics(rl_job_name, ['episode_reward_min']).dataframe()\n",
    "    df_max = TrainingJobAnalytics(rl_job_name, ['episode_reward_max']).dataframe()\n",
    "    df['rl_reward_mean'] = df['value']\n",
    "    df['rl_reward_min'] = df_min['value']\n",
    "    df['rl_reward_max'] = df_max['value']\n",
    "    num_metrics = len(df)\n",
    "\n",
    "    if num_metrics == 0:\n",
    "        print(\"No algorithm metrics found in CloudWatch\")\n",
    "    else:\n",
    "        plt = df.plot(x='timestamp', y=['rl_reward_mean'], figsize=(18,6), fontsize=18, legend=True, style='-', color=['b','r','g'])\n",
    "        plt.fill_between(df.timestamp, df.rl_reward_min, df.rl_reward_max, color='b', alpha=0.2)\n",
    "        plt.set_ylabel('Mean reward per episode', fontsize=20)\n",
    "        plt.set_xlabel('Training time (s)', fontsize=20)\n",
    "        plt.legend(loc=4, prop={'size': 20})\n",
    "else:\n",
    "    print(\"Can't plot metrics in local mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
